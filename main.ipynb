{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, syllable_count, hvc_neurons, syllable_features):\n",
    "        super(Model, self).__init__()\n",
    "        '''\n",
    "        Purpose:\n",
    "            - Initializes the song bird circuit \n",
    "            \n",
    "        Args:\n",
    "            - \n",
    "        \n",
    "        Returns:\n",
    "            - \n",
    "        '''\n",
    "\n",
    "        self.HVC_size = syllable_count\n",
    "        \n",
    "        # # psuedo RNN\n",
    "        # self.HVC = nn.Linear((syllable_count + hvc_neurons), hvc_neurons)\n",
    "        self.HVC = nn.Linear(syllable_count, self.HVC_size, bias=False)\n",
    "\n",
    "        self.RA = nn.Linear(self.HVC_size, syllable_features, bias=False)\n",
    "\n",
    "        self.LMAN_Size = syllable_features * 2\n",
    "        self.Area_X_size = self.HVC_size * self.LMAN_Size\n",
    "\n",
    "        # Technically, Area_X has RA * HVC amount of neurons, but for simplicity, we will assume that it is just RA * 1\n",
    "        self.Area_X =  nn.Linear(self.HVC_size, self.Area_X_size, bias=False)\n",
    "        self.LMAN =  nn.Linear(self.Area_X_size, self.LMAN_Size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.clock = 0 \n",
    "\n",
    "    # def forward_HVC(self, syllable, hvc_neurons):\n",
    "    #     x = self.HVC(torch.cat((syllable, hvc_neurons)))\n",
    "    #     return F.relu(x)\n",
    "\n",
    "    def forward_HVC(self, syllable):\n",
    "        x = self.HVC(syllable)\n",
    "        return F.relu(x)\n",
    "    \n",
    "    def forward_RA(self, hvc_neurons):\n",
    "        x = self.RA(hvc_neurons)\n",
    "        return F.relu(x)\n",
    "    \n",
    "    def forward_Area_X(self, hvc_neurons):\n",
    "        x = self.Area_X(hvc_neurons)\n",
    "        return F.relu(x)\n",
    "    \n",
    "    def forward_LMAN(self, hvc_neurons):\n",
    "        x = self.LMAN(hvc_neurons)\n",
    "        return F.relu(x)\n",
    "    \n",
    "    def set_weights(self, layer, weight_matrix):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            - Sets the weights of the layer\n",
    "        \n",
    "        Args:\n",
    "            - layer: the layer to set the weights of\n",
    "            - weight_matrix: the weight matrix to set the weights to\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        weight_matrix = nn.Parameter(weight_matrix)\n",
    "\n",
    "        if layer == \"HVC\":\n",
    "            self.HVC.weight = weight_matrix\n",
    "        if layer == \"RA\":\n",
    "            self.RA.weight = weight_matrix\n",
    "        if layer == \"Area_X\":\n",
    "            self.Area_X.weight = weight_matrix\n",
    "        if layer == \"LMAN\":\n",
    "            self.LMAN.weight = weight_matrix\n",
    "        \n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            - removes some weights between HVC_AreaX \n",
    "                - in areaX there are LMAN * HVC number of neurons\n",
    "                - each areaX neuron is connected to a unique combination of LMAN and HVC neurons\n",
    "            - removes weights between AreaX and LMAN\n",
    "                - many areaX neurons converge on a single LMAN neuron \n",
    "            - sets syllable to HVC weights to one (also one to one mapping)\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # set all weights between syllable and HVC to 0\n",
    "        self.set_weights(\"HVC\", torch.zeros(self.HVC_size, self.HVC_size))\n",
    "\n",
    "        for i in range(self.HVC_size):\n",
    "            for j in range(self.HVC_size):\n",
    "                if i == j:\n",
    "                    # get weight matrix of HVC\n",
    "                    weight_matrix = self.HVC.weight\n",
    "                    weight_matrix = torch.tensor(weight_matrix)\n",
    "                    weight_matrix[i][j] = torch.tensor([1.0])\n",
    "                    self.set_weights(\"HVC\", weight_matrix)\n",
    "\n",
    "\n",
    "        # set all weights between HVC and AreaX to 0\n",
    "        self.set_weights(\"Area_X\", torch.zeros(self.Area_X_size, self.HVC_size))\n",
    "\n",
    "        for i in range(self.Area_X_size):\n",
    "            for j in range(self.HVC_size):\n",
    "                # hard to explain see picture on my phone from apr 30 2023\n",
    "                if j == (i % self.HVC_size): \n",
    "                    # get weight matrix of AreaX\n",
    "                    weight_matrix = self.Area_X.weight\n",
    "                    weight_matrix = torch.tensor(weight_matrix)\n",
    "                    weight_matrix[i][j] = torch.rand(1)\n",
    "                    self.set_weights(\"Area_X\", weight_matrix)\n",
    "\n",
    "        # set all weights between AreaX and LMAN to 0\n",
    "        self.set_weights(\"LMAN\", torch.zeros(self.LMAN_Size, self.Area_X_size))\n",
    "\n",
    "        for i in range(self.LMAN_Size):\n",
    "            start = i * self.HVC_size\n",
    "            stop = start + self.HVC_size\n",
    "            for j in range(start, stop):\n",
    "                weight_matrix = self.LMAN.weight\n",
    "                weight_matrix = torch.tensor(weight_matrix)\n",
    "                weight_matrix[i][j] = torch.tensor([1.0])\n",
    "                self.set_weights(\"LMAN\", weight_matrix)\n",
    "              \n",
    "\n",
    "    def hebbian_update(self, pre_layer_activations, post_layer_activations, post_layer_weights, learning_rate):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            - if the pre_layer and post_layer fire in the same cycle, then the weights between them are updated by the learning rate in positive direction\n",
    "            - if not, then the weights are updated in negative direction by the learning rate\n",
    "\n",
    "        Args:\n",
    "            - pre_layer_activations: the activations of the pre layer\n",
    "            - post_layer_activations: the activations of the post layer\n",
    "\n",
    "        Returns:\n",
    "            - post_weights: the updated weights between the pre and post layer (belongs to the post layer)\n",
    "        \"\"\"\n",
    "\n",
    "        post_weights = post_layer_weights\n",
    "\n",
    "        for i in range(len(post_layer_activations)):\n",
    "            for j in range(len(pre_layer_activations)):\n",
    "                if pre_layer_activations[j] != 0 and post_layer_activations[i] != 0:\n",
    "                    post_weights[i][j] += learning_rate\n",
    "                else:\n",
    "                    post_weights[i][j] -= learning_rate\n",
    "\n",
    "        return post_weights \n",
    "\n",
    "    def HVC_X_update_rule(self, area_x_weights, reward, learning_rate, LMAN_activation):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            - updates the weights between HVC and AreaX based on the reward\n",
    "            - delta_hvc_x_i = Lman_i * (reward) * lr\n",
    "\n",
    "        Args:\n",
    "            - area_x_weights: the weights between HVC and AreaX\n",
    "            - reward: the reward signal\n",
    "            - learning_rate: the learning rate\n",
    "\n",
    "        Returns:\n",
    "            - updated_weights: the updated weights between HVC and AreaX\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(self.Area_X_size):\n",
    "            for j in range(self.HVC_size):\n",
    "                if j == (i % self.HVC_size):\n",
    "                    LMAN_Neuron = i // self.HVC_size\n",
    "                    area_x_weights[i][j] = area_x_weights[i][j] + LMAN_activation[LMAN_Neuron] * reward * learning_rate\n",
    "\n",
    "        return area_x_weights\n",
    "\n",
    "    def visualize_layer_weights(self, layer):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            - Visualizes the weights of the layer\n",
    "\n",
    "        Args:\n",
    "            - layer: the layer to visualize\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        if layer == \"HVC\":\n",
    "            weights = self.HVC.weight\n",
    "        elif layer == \"RA\":\n",
    "            weights = self.RA.weight\n",
    "        elif layer == \"Area_X\":\n",
    "            weights = self.Area_X.weight\n",
    "        elif layer == \"LMAN\":\n",
    "            weights = self.LMAN.weight\n",
    "        \n",
    "        weights = weights.detach().numpy()\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(weights, cmap='coolwarm')\n",
    "\n",
    "        # Add a colorbar to the heatmap\n",
    "        cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "        # Add value labels to the heatmap\n",
    "        for i in range(weights.shape[0]):\n",
    "            for j in range(weights.shape[1]):\n",
    "                ax.text(j, i, '{:.2f}'.format(weights[i, j]), ha='center', va='center', color='w')\n",
    "\n",
    "        # Add a title and axis labels to the heatmap\n",
    "        ax.set_title('Weight Matrix Heatmap')\n",
    "        ax.set_xlabel('Input Features')\n",
    "        ax.set_ylabel('Output Features')\n",
    "\n",
    "        # Show the heatmap\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def print_layer_weights(self, layer):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            - Prints the weights of the layer in a heatmap format\n",
    "\n",
    "        Args:\n",
    "            - layer: the layer to print\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        if layer == \"HVC\":\n",
    "            weights = self.HVC.weight\n",
    "        if layer == \"RA\":\n",
    "            weights = self.RA.weight\n",
    "        if layer == \"Area_X\":\n",
    "            weights = self.Area_X.weight\n",
    "        if layer == \"LMAN\":\n",
    "            weights = self.LMAN.weight\n",
    "        \n",
    "        print(weights)\n",
    "\n",
    "    def negative_MSE_reward(self, network_output, target, scaling_factor):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            - Calculates the negative MSE reward for the network\n",
    "\n",
    "        Args:\n",
    "            - Network output\n",
    "            - Target\n",
    "\n",
    "        Returns:   \n",
    "            - reward: the negative MSE reward\n",
    "        \"\"\"\n",
    "\n",
    "        reward = -scaling_factor * F.mse_loss(network_output, target)\n",
    "        return reward\n",
    "    \n",
    "    def MSE(self, output, target):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            - Calculates the MSE between the output and target\n",
    "\n",
    "        Args:\n",
    "            - output\n",
    "            - target\n",
    "\n",
    "        Returns:\n",
    "            - MSE\n",
    "        \"\"\"\n",
    "\n",
    "        return F.mse_loss(output, target)\n",
    "    \n",
    "    def LMAN_activation_to_noise(self, LMAN_output):\n",
    "        \"\"\"\n",
    "        Purpose:\n",
    "            - Adds noise to the LMAN activations (rn just a normal distribution)\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            - Noise vector \n",
    "        \"\"\"\n",
    "\n",
    "        # normal distribution with mean 0 and std 1\n",
    "        noise = torch.randn(len(LMAN_output))\n",
    "\n",
    "        LMAN_noise = LMAN_output + noise\n",
    "\n",
    "        return LMAN_noise, noise\n",
    "    \n",
    "    def LMAN_RA_bias(self, RA, LMAN_bias):\n",
    "        new_RA = torch.zeros(len(RA))\n",
    "        bias_total = 0\n",
    "        for i in range(len(RA)):\n",
    "            for j in range(i*2, (i*2)+2):\n",
    "                bias_total += LMAN_bias[j]\n",
    "\n",
    "            new_RA[i] = RA[i] + bias_total\n",
    "        \n",
    "        print(f\"RA inside LMAN_RA_bias: {RA}\")\n",
    "        return RA\n",
    "    \n",
    "    def get_weights(self, layer):\n",
    "        if layer == \"HVC\":\n",
    "            weights = self.HVC.weight\n",
    "        elif layer == \"RA\":\n",
    "            weights = self.RA.weight\n",
    "        elif layer == \"Area_X\":\n",
    "            weights = self.Area_X.weight\n",
    "        elif layer == \"LMAN\":\n",
    "            weights = self.LMAN.weight\n",
    "\n",
    "        return torch.tensor(weights)\n",
    "    \n",
    "    def reward_prediction_error(self, without_LMAN, with_LMAN, target):\n",
    "        without_LMAN_error = self.MSE(without_LMAN, target)\n",
    "        with_LMAN_error = self.MSE(with_LMAN, target)\n",
    "\n",
    "        reward_prediction_error = with_LMAN_error - without_LMAN_error\n",
    "\n",
    "        return reward_prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_env():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def generate_syllables(self, syllable_count):\n",
    "        '''\n",
    "        Purpose: \n",
    "            - Generate syllables for training\n",
    "            - 4 params for syllable generation, freq, amplitude, y-offset, length\n",
    "        Input: \n",
    "            - Number of syllables to generate\n",
    "        Output: \n",
    "            - returns songs as a numpy array of shape (syllable_count * individual lengths, 3)\n",
    "            - returns length of each syllable \n",
    "        ''' \n",
    "\n",
    "        self.syllable_count = syllable_count\n",
    "\n",
    "        # careful, the code below is cringe af!\n",
    "        syllable_duration = np.random.uniform(3, 10, syllable_count)\n",
    "        syllable_duration = np.array(syllable_duration, dtype=int)\n",
    "        self.syllable_lengths = syllable_duration\n",
    "\n",
    "        total_syllable_length = np.sum(syllable_duration)\n",
    "\n",
    "        syllables = np.zeros((total_syllable_length, 3))\n",
    "        self.syllable_index = np.zeros((total_syllable_length, 1))\n",
    "        \n",
    "        cum_length = 0\n",
    "        for i, length in enumerate(syllable_duration):\n",
    "            self.syllable_index[cum_length:cum_length+length] = i\n",
    "            freq = np.random.uniform(0, 1)\n",
    "            amp = np.random.uniform(0, 1)\n",
    "            y_offset = np.random.uniform(0, 1)\n",
    "            for t in range(length):\n",
    "                syllables[cum_length, 0] = freq\n",
    "                syllables[cum_length, 1] = amp\n",
    "                syllables[cum_length, 2] = y_offset\n",
    "                cum_length += 1\n",
    "\n",
    "        self.syllables = syllables\n",
    "\n",
    "        self.num_syllables = syllable_count\n",
    "\n",
    "        return syllables, syllable_duration\n",
    "\n",
    "    def generate_train_set(self, train_size):\n",
    "        '''\n",
    "        Purpose:\n",
    "            - Generate a training set for the model\n",
    "        Input:\n",
    "            - train_size: number of training examples to generate\n",
    "        Output:\n",
    "            - train_set: a list of training examples\n",
    "        '''\n",
    "        output_set = []\n",
    "        input_set = []\n",
    "\n",
    "        for i in range(train_size):\n",
    "            # self.syllables.shape[0] is the the total num of timesteps \n",
    "            padded_syllables = np.zeros((self.syllables.shape[0], 3))\n",
    "            padded_HVC_on = np.zeros((self.syllables.shape[0], self.syllable_count))\n",
    "\n",
    "    \n",
    "            lower_bound_syllable_index = np.random.randint(0, self.num_syllables)\n",
    "            upper_bound_syllable_index = np.random.randint(lower_bound_syllable_index, self.num_syllables)\n",
    "\n",
    "            if lower_bound_syllable_index != upper_bound_syllable_index:\n",
    "                list_of_syllables_indices = [i for i in range(lower_bound_syllable_index, upper_bound_syllable_index)]\n",
    "            else:\n",
    "                list_of_syllables_indices = [lower_bound_syllable_index]\n",
    "\n",
    "            # create a list of first timestep of each syllable\n",
    "            list_of_first_timestep_per_syllable = []\n",
    "\n",
    "            for syllable_index in list_of_syllables_indices:\n",
    "                first_timestep = np.sum(self.syllable_lengths[:syllable_index])\n",
    "                list_of_first_timestep_per_syllable.append(first_timestep)\n",
    "\n",
    "        \n",
    "            # set the HVC_on to 1 for the interval\n",
    "            for i, syllable in enumerate(list_of_syllables_indices):\n",
    "                padded_HVC_on[list_of_first_timestep_per_syllable[i]:list_of_first_timestep_per_syllable[i]+self.syllable_lengths[syllable], syllable] = 1\n",
    "\n",
    "            for syllable in list_of_syllables_indices:\n",
    "                padded_syllables[list_of_first_timestep_per_syllable[i]:list_of_first_timestep_per_syllable[i]+self.syllable_lengths[syllable], :] = self.syllables[list_of_first_timestep_per_syllable[i]:list_of_first_timestep_per_syllable[i]+self.syllable_lengths[syllable], :]\n",
    "          \n",
    "            # padded to tensor\n",
    "            padded_syllables = torch.from_numpy(padded_syllables).float()\n",
    "            padded_HVC_on = torch.from_numpy(padded_HVC_on).float()\n",
    "\n",
    "            output_set.append(padded_syllables)\n",
    "            input_set.append(padded_HVC_on)\n",
    "\n",
    "        return output_set, input_set\n",
    "\n",
    "    def get_syllables(self):\n",
    "        return self.syllables\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Grounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy target: tensor([0.6540, 0.4080])\n",
      "RA inside LMAN_RA_bias: tensor([0.1708, 0.4116], grad_fn=<ReluBackward0>)\n",
      "ra_output without LMAN: tensor([0.1708, 0.4116], grad_fn=<ReluBackward0>)\n",
      "RA_output with LMAN: tensor([0.1708, 0.4116], grad_fn=<ReluBackward0>)\n",
      "rpe: 0.0\n",
      "RA inside LMAN_RA_bias: tensor([0.2708, 0.5116], grad_fn=<ReluBackward0>)\n",
      "ra_output without LMAN: tensor([0.2708, 0.5116], grad_fn=<ReluBackward0>)\n",
      "RA_output with LMAN: tensor([0.2708, 0.5116], grad_fn=<ReluBackward0>)\n",
      "rpe: 0.0\n",
      "RA inside LMAN_RA_bias: tensor([0.3708, 0.6116], grad_fn=<ReluBackward0>)\n",
      "ra_output without LMAN: tensor([0.3708, 0.6116], grad_fn=<ReluBackward0>)\n",
      "RA_output with LMAN: tensor([0.3708, 0.6116], grad_fn=<ReluBackward0>)\n",
      "rpe: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/0yhgng1n5k12_prdv1mgd4hw0000gn/T/ipykernel_1843/631944864.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weight_matrix = torch.tensor(weight_matrix)\n",
      "/var/folders/s5/0yhgng1n5k12_prdv1mgd4hw0000gn/T/ipykernel_1843/631944864.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weight_matrix = torch.tensor(weight_matrix)\n",
      "/var/folders/s5/0yhgng1n5k12_prdv1mgd4hw0000gn/T/ipykernel_1843/631944864.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weight_matrix = torch.tensor(weight_matrix)\n",
      "/var/folders/s5/0yhgng1n5k12_prdv1mgd4hw0000gn/T/ipykernel_1843/631944864.py:321: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(weights)\n"
     ]
    }
   ],
   "source": [
    "# ignore for now\n",
    "hvc_neurons = 1\n",
    "\n",
    "reward_history = []\n",
    "RA_output_history = []\n",
    "\n",
    "syllable_features = 2\n",
    "syllable_count = 1\n",
    "epochs = 3\n",
    "\n",
    "model = Model(syllable_count, hvc_neurons, syllable_features)\n",
    "\n",
    "# initial hidden state\n",
    "hidden_state = torch.zeros(hvc_neurons)\n",
    "\n",
    "dummy_HVC_input = torch.ones((syllable_count))\n",
    "dummy_target = torch.rand((syllable_features))\n",
    "print(f\"dummy target: {dummy_target}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # HVC -> RA -> RA_output\n",
    "    hvc_state = model.forward_HVC(dummy_HVC_input)\n",
    "    ra_output = model.forward_RA(hvc_state)\n",
    "\n",
    "    ra_output_without_LMAN = ra_output\n",
    "\n",
    "    # HVC -> Area_X -> LMAN (random normal for now)\n",
    "    Area_X_output = model.forward_Area_X(hvc_state)\n",
    "    LMAN_output = model.forward_LMAN(Area_X_output)\n",
    "\n",
    "    # # add normal distribution noise to LMAN output\n",
    "    # LMAN_output = LMAN_output + torch.normal(mean=0, std=0.1, size=LMAN_output.shape)\n",
    "\n",
    "    # RA_output = RA_output + LMAN bias \n",
    "    RA_output = model.LMAN_RA_bias(ra_output, LMAN_output)\n",
    "    RA_output_history.append(RA_output.detach().numpy())\n",
    "\n",
    "    print(f\"ra_output without LMAN: {ra_output_without_LMAN}\")\n",
    "    print(f\"RA_output with LMAN: {RA_output}\")\n",
    "\n",
    "    # # calculate reward \n",
    "    # reward = model.negative_MSE_reward(RA_output, dummy_target, scaling_factor=.1)\n",
    "    # reward_history.append(reward.detach().numpy())\n",
    "    \n",
    "    rpe = model.reward_prediction_error(ra_output_without_LMAN,RA_output, dummy_target )\n",
    "\n",
    "    print(f\"rpe: {rpe}\")\n",
    "\n",
    "    # hebbian update\n",
    "    weight_matrix = model.hebbian_update( hvc_state,RA_output, post_layer_weights=model.get_weights(\"RA\"), learning_rate=0.1)\n",
    "\n",
    "    model.set_weights(\"RA\", weight_matrix)\n",
    "\n",
    "    # areaX update\n",
    "    weight_matrix = model.HVC_X_update_rule(area_x_weights=model.get_weights(\"Area_X\"), LMAN_activation=LMAN_output, reward=rpe, learning_rate=0.1)\n",
    "    model.set_weights(\"Area_X\", weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reward history\n",
    "plt.plot(RA_output_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
